\documentclass[12pt,onecolumn]{IEEEtran}


\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{stfloats}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{psfrag}
%\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{array}
\usepackage{epstopdf}
\usepackage{authblk}
\usepackage{graphicx} 
\usepackage{amsthm} 
\usepackage{lipsum}
\usepackage{verbatim} 
\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}

\newcommand{\blue}{\color{blue}}
\newcommand{\black}{\color{black}}
\newcommand{\red}{\color{red}}

\usepackage{graphicx}
\newcommand{\spaceblank}{\vskip 4mm}

\renewcommand{\baselinestretch}{1.4}

\newtheorem{Definition}{Definition}
\newtheorem{Problem}{Problem}
\newtheorem{Lemma}{Lemma}
\newtheorem{Theorem}{Theorem}
\newtheorem{Algorithm}{Algorithm}
\newtheorem{Policy}{Policy}
\newtheorem{Scheme}{Scheme}
\newtheorem{Scenario}{Scenario}
\newtheorem{Assumption}{Assumption}
\newtheorem{Proposition}{Proposition}
\newtheorem{Remark}{Remark}
\newtheorem{Solution}{Solution}
\newtheorem{Baseline}{Baseline}
\newtheorem{Example}{Example}
\newtheorem{Corollary}{Corollary}
\newtheorem{Model}{Model}

\begin{document}
\title{Reply to the Editor and Reviewers' Comments on Manuscript ID VT-2019-02643}

\author{}
\maketitle

We thank the editor and reviewers for all the constructive comments. They have helped to improve the technical accuracy and presentation of the manuscript. In the revised manuscript, the main changes are emphasized in {\blue blue} for the reading convenience.

In this reply file, we first state the comments in {\em \black italics and black}, and then respond to them in {\color{blue}blue}. Unless mentioned otherwise, the equation, figure and citation numbers refer to those in this reply file.


\section{Response to Reviewer 1}
\spaceblank
\noindent{\em \textbf{Comment 1:}  The motivation of this paper has not been discussed in detail. Please revise the section.
}
\spaceblank	

{\blue \textbf{Response R1-1:} Thank you for the comment. We have revised the Introduction section (Section 1) of the manuscript to highlight our motivations. To our best knowledge, this is  the first work considering the uplink scheduling scenario in an infinite time horizon when the active users come at random locations and random time instances to request uplink transmission of finite data. Since there is no existing optimization tools suitable for this new scheduling scenario, we propose a new solution framework with low-complexity and analyzable performance. The motivations are further explained below.

First of all, existing works mainly consider the scenarios where either single user \cite{You2015SingleUserWPT,you2016singleuser,huang2012dynamic,liu2016delayopt} or multiple users \cite{you2016multiuser,chen2015decentalizedgame,chen2016gametheorymec,mao2016power-delay,Ko2018HetnetMEC} at fixed locations offload computation tasks via uplink. Thus, the arrival of new offloading users or the departure of existing offloading users is excluded in these scenarios. In practice, when the computation task of a mobile user is completed, the mobile user may become inactive and new users with computation tasks may join the system in a stochastic manner. To the best of our knowledge, our work is the first attempt to deal with the optimization problems in the MEC systems with random user arrivals in both spatial and temporal domains.

Secondly, the MDP problem formulated from the above new scenario cannot be solved efficiently by conventional MDP solution algorithms such as value iteration or policy iteration, neither can it be solved by popular approximate MDP approaches such as linear approximation approaches developed in \cite{YCui2010AMDP-OFDM-downlink,YCui2010AMDP-OFDM-uplink,RWang2011DistTowHopMIMO,RWang2011QueueAwareCoorp, RWang2013RelayApproxMDP,YSun2019PushingCaching,BLv2019Cache} or deep reinforcement learning approaches raised in \cite{XQiu2019DRL-MEC,LTan2018DRL-Cache-MEC,JWang2019ResorceAlloc-MEC,YLiu2019DRL-VehicleEdge,YHe2018cache-comp-DRL}. The reasons are provided below.
\begin{itemize}
\item In the uplink transmission scenario with fixed users, there are fixed uplink transmission queues in the system, and the dynamics come from the packet  arrival and departure of all the queues. Suppose there are $Q$ uplink queues and each queue can buffer at most $P$ packets, the total number of system states is $P^Q$. Although the complexity is exponential, there are existing sub-optimal low-complexity approximate MDP methods to solve the scheduling problem as in \cite{YCui2010AMDP-OFDM-downlink,YCui2010AMDP-OFDM-uplink,RWang2011DistTowHopMIMO,RWang2011QueueAwareCoorp, RWang2013RelayApproxMDP,YSun2019PushingCaching,BLv2019Cache}. 
\item However, when random user arrival is allowed in the system, the number of system states is not even countable. First of all, the number of users in the system is variable. Even there is only one user in the system, it can appear in arbitrary location of the cell, leading to different uplink power consumption. Hence, the total number of system state, enumerating all the possible user numbers and user locations, is not countable. Hence, the scheduling scenario with random user arrivals is significantly different from and of much larger complexity than the scenario with fixed users. The conventional approximate MDP methods cannot be applied. This motivates us to propose a new approximate MDP method in this work. 
\end{itemize}

Finally, we would also like to shed some lights on the optimization theory of approximate MDP in this work. Most of the existing approximate MDP methods rely on numerical value iteration to evaluate the approximate value function, and it is usually difficult to investigate the performance of the proposed algorithm analytically. In this work, we manage to establish a new solution framework where (1) the approximate value function can be calculated via analytical expression, and (2) the average system cost can be bounded analytically.
}

\spaceblank
\noindent{\em \textbf{Comment 2:} Given a wide variety of works in edge computing, we can see a lot of scheduling algorithms for it. Hence, I can not see any proper justification for proposing another scheduling framework.
}	
\spaceblank
{\blue \textbf{Response R1-2:} Thank you for your comment. There are indeed a variety of works in edge computing, e.g. \cite{You2015SingleUserWPT,you2016multiuser,chen2015decentalizedgame,chen2016gametheorymec,huang2012dynamic,mao2016dynamicmec,mao2016power-delay,Wang2019JointMEC}, and etc. As explained in \textbf{Response R1-1}, however, all these works considered the uplink offloading scenarios with fixed single or multiple users. The scheduling algorithms proposed in these existing works are not applicable to the scenario where the number and locations of users in the MEC system are varying. To our best knowledge, our work is the first attempt to deal with the uplink scheduling problem with random user arrivals, which naturally necessitates new scheduling frameworks (as explained in \textbf{Response R1-1}).
}


\spaceblank
\noindent{\em\textbf{Comment 3:} The contributions of this paper are very limited. I can not see enough contributions with respect to the published conference paper.
}

\spaceblank
{\blue \textbf{Response R1-3:} Thank you for your comment. We have revised the manuscript essentially by proposing a new baseline policy with better performance. The power adaptation algorithm and reinforcement learning algorithm are also revised accordingly. Specifically, compared with the conference paper, the main contributions of this manuscript are summarized below.

\begin{figure}[tb]
	\centering
	\includegraphics[scale=0.6]{cost_size.eps}
	\caption{Average per-device costs versus task size for different policies, where $P_N=0.1$, $p_r=2.8\times 10^{-9}$ W, initial system state $\mathbf S_1=(\mathcal U_E(t)=\emptyset, \mathcal U_L(t)=\emptyset, I_N(t)=0)$, ``BSL, K=1" refers to the baseline policy in the original manuscript and our conference paper, ``BSL, K=4" refers to the new baseline policy in the revised manuscript, ``Proposed, K=1" and ``Proposed, K=4" refer to the proposed policies evolved from ``BSL, K=1" and ``BSL, K=4" respectively.}
	\label{fig:cost_size}
\end{figure}

% \begin{figure}[tb]
% 	\centering
% 	\includegraphics[scale=0.6]{sgd_curve.eps}
% 	\caption{Convergence of the SGD algorithm, where $P_N=0.1$.}
% 	\label{fig:sgd_curve}
% \end{figure}

\begin{figure}[tb]
	\centering
	\includegraphics[scale=0.6]{cost_pr.eps}
	\caption{Performance gain of the optimized $p_r^*$, where $K=4$, $P_N=0.1$, $p_r^*=3.6\times 10^{-9}$ W, initial system state $\mathbf S_1=(\mathcal U_E(t)=\emptyset, \mathcal U_L(t)=\emptyset, I_N(t)=0)$, ``BSL, $p_r=1\times 10^{-9}$ W'' refers to the baseline policy with initial $p_r=1\times 10^{-9}$ W, ``BSL, $p_r=3.6\times 10^{-9}$ W'' refers to the baseline policy with optimized $p_r^*=3.6\times 10^{-9}$ W, ``Proposed, $p_r=1\times 10^{-9}$ W'' and ``Proposed, $p_r=3.6\times 10^{-9}$ W'' refer to the proposed policies evolved from ``BSL, $p_r=1\times 10^{-9}$ W'' and ``BSL, $p_r=3.6\times 10^{-9}$ W'' respectively.}
	\label{fig:cost_pr}
\end{figure}

\begin{figure}[tb]
	\centering
	\includegraphics[scale=0.6]{mismatch.eps}
	\caption{Performance with initial guess $P_N=0.2$ and learned $P_N=0.1$, where $p_r=2.8\times 10^{-9}$ W, initial system state $\mathbf S_1=(\mathcal U_E(t)=\emptyset, \mathcal U_L(t)=\emptyset, I_N(t)=0)$, ``Initial $P_N$'' refers to the performance with initial inaccurate $P_N=0.2$, ``Learned $P_N$'' refers to the performance with the learned accurate $P_N=0.1$.}
	\label{fig:mismatch}
\end{figure}


\begin{itemize}
\item \textbf{A new baseline policy with better performance.} In the revised journal version, we extend the baseline policy of the conference version to a more general policy. In the baseline policy of the conference version, at most one active edge computing device is allowed in the system. Thus, a new active device is scheduled for edge computing if and only if there is currently no edge computing device in the system. While in the revised journal version, we generalize the baseline policy by allowing at most $K$ active edge computing devices in the system. This generalization is non-trivial as we should consider the transition probabilities between different numbers of edge computing devices (there is no such issue in the conference version as only one edge computing device is allowed in the baseline policy of the conference version). Significant efforts are spent to derive the value function of the new baseline policy. Simulation result in Fig. \ref{fig:cost_size} (Fig. 6 of the revised manuscript) demonstrates that the revised baseline policy with $K=4$ has better performance than the original baseline policy ($K=1$). Better baseline policy also leads to better proposed policy after one-step policy iteration. For example in Fig. \ref{fig:cost_size}, the proposed policy with $K=4$ (obtained via one-step policy iteration from the new baseline policy with $K=4$) is better than the proposed policy with $K=1$ (obtained via one-step policy iteration from the original baseline policy). Moreover, the new baseline policy can also provide a tighter analytical cost upper bound on the proposed policy.



\item \textbf{An efficient algorithm to optimize the transmission power of the baseline policy without knowledge of user distribution statistics.} The performance of the baseline policy,  as well as the proposed policy evolved from the baseline policy,  depends heavily on the uplink transmission power. We develop a stochastic gradient descent (SGD) algorithm to optimize the transmission power without system statistics in an online manner, such that the performance of both baseline and proposed policies can be improved. In the conference version, we fixed the uplink transmission power. The performance gain of this optimization is illustrated in Fig. \ref{fig:cost_pr} (Fig. 11 of the revised manuscript), where the optimized $p_r^*$ lead to better performance in both baseline policy and the proposed policy.

\item \textbf{An efficient reinforcement learning algorithm without knowledge of user distribution statistics.} In this work, the approximate value function depends on the statistics of new active devices which may not be known to the BS in practice. In the conference version, we do not consider this issue, and simply make assumptions on the distribution of new active devices. This will cause the distribution mismatch if the assumption is not accurate. In order to address this issue, we design a novel and efficient reinforcement learning method for evaluating the value function in the journal version. The conventional reinforcement learning method, i.e. Q-learning, needs to learn the Q-function value for all state-action pairs, which is infeasible in our problem due to the tremendous state and action spaces. In the proposed reinforcement learning method, we only need to track the statistical parameters in the expression of approximate value function via online observations. The learning efficiency is significantly improved. Moreover, the learning of statistical parameters can greatly improve the performance of the proposed policy. For example, the performance gain from tracking the user arrival rate is illustrated in Fig. \ref{fig:mismatch} (Fig. 12 of the revised manuscript), where the curve with $P_N=0.2$ refers to the proposed policy with initial inaccurate value of $P_N$, and the curve with $P_N=0.1$ refers to the proposed policy with learned accurate value of $P_N$.
\end{itemize}
}

% Firstly, the main contributions of this work have been summarized in the manuscript and we restate them here:
% \begin{itemize}
% 	\item \textbf{A novel approximate MDP framework:} Due to the dynamics of user arrival and departure, the number of mobile devices in the MEC system is variable. The system state space should enumerate all the possible numbers of mobile devices. The conventional approximate MDP approaches in \cite{YCui2010AMDP-OFDM-downlink,YCui2010AMDP-OFDM-uplink,RWang2011DistTowHopMIMO,RWang2011QueueAwareCoorp, RWang2013RelayApproxMDP,YSun2019PushingCaching,BLv2019Cache}, which are designed for fixed users, cannot be applied to address the curse of dimensionality. Thus, a novel solution framework is proposed in this paper. Particularly, we first propose a baseline scheduling policy, whose value function can be derived analytically. Then, one-step policy iteration is applied based on the value function of the baseline policy to obtain a proposed sub-optimal policy.
% 	\item \textbf{Efficient reinforcement learning algorithm design:} Since the value function depends on some system statistics which may not be known in practice, we design a novel reinforcement learning method for evaluating the value function. The conventional reinforcement learning method, i.e. Q-learning, needs to learn the Q-function for all state-action pairs, which is infeasible in our problem due to the tremendous state and action spaces. In the proposed reinforcement learning method, we only track some parameters in the value function. The learning efficiency is significantly improved.
% 	\item \textbf{SGD-based policy improvement:} The performance of the baseline policy,  as well as the proposed policy evolved from the baseline policy,  depends heavily on the transmission parameter. We design a stochastic gradient descent (SGD) algorithm to optimize the transmission parameter with unknown system statistics in an online manner, such that the performance of both baseline and proposed policies can be improved.
% 	\item \textbf{Analytical performance bound:} For a conventional approximate MDP method, e.g., via the parametric approximation architectures \cite{Bertsekas2012Dynamic}, it is hard to investigate the performance analytically due to the use of approximate value function. In our proposed method, however, it is proved that the derived value function of the baseline policy is a cost upper bound of the proposed policy. Thus, the worst-case performance of the proposed policy can be obtained analytically. 
% \end{itemize}

% Secondly, the differences and new contributions compared with the conference version are summarized below:
% \begin{itemize}
%     \item In the conference version, in order to evaluate the value function, we assume the arrival rate of new active devices $P_N$ , the distributions of the locations of new arriving active devices $\lambda(\mathbf{l})$, the CPU cycles needed to compute one bit of input task data $\pi_{\ell}(\ell)$ and CPU frequency $\pi_f(f)$ are known a priori, which may not be the case in practice. Hence, in the journal version, we make no assumptions on these distributions, and an efficient reinforcement learning algorithm is proposed to tackle the unknown distributions.
%     \item In the conference version, the average receiving power $p_r$ in the baseline policy is chosen without any optimization, which may lead to poor performance of the baseline policy. In turn, it may deteriorate the performance of our proposed policy which is derived based on the baseline policy. In the journal version, we leverage a gradient descent approach to optimize $p_r$, and a stochastic gradient descent (SGD) based algorithm is proposed due to the unknown system statistics. The optimized $p_r$ can be used to improve the baseline policy, and in turn, to improve our proposed policy.
%     \item In the conference version, the simulation results only show the impacts of arrival rate on the performance. In the journal version, we conduct extensive simulations and generate more illustrative simulation results to demonstrate the superiority of our proposed policy. Specifically, the impacts of task size on the performance of different policies are shown, which further affirms the effectiveness of our proposed policy. Also, the latency and power consumption distributions for different arrival rates are shown to better reflect how the system adapts to different arrival rates. Moreover, the reinforcement learning and SGD convergence curves are drawn to show the efficiency and effectiveness of the learning and SGD algorithms. Finally, the performances of using different $p_r$ are compared to demonstrate the improvement of using the $p_r$ optimized via SGD.
% \end{itemize}

% Finally, to further differentiate from the conference version and better solidify this work, more contributions are added in the revised version where the baseline policy exploited to derive the approximate value function is modified to a more general form. The details are elaborated in the following.

% The generalized baseline policy is given below.
% \begin{Policy}[Baseline Scheduling Policy $\Pi$] Given the system state $\mathbf{S}_t$, the  baseline scheduling policy $\Pi(\mathbf{S}_t)=\left(a_t, p(t), e_t\right)$ is provided below.
%     \begin{itemize}
%     \item Uplink transmission device selection $a_t = \min \mathcal{U}_E(t)$, $\forall t$. Thus, the BS schedules the uplink device in a first-come-first-serve manner.
%     \item The transmission power $p(t)$ compensates the large-scale fading (link compensation). Thus,
%     \begin{align}
%     p(t)=\frac{p_r}{\rho_{a_t}}, \forall t,
%     \end{align}
%     where $p_r$ is the average receiving power at the BS.
%     {\item  The task of the new active device is offloaded to MEC server only when there are less than $K$ active edge computing device in the system, i.e.,
%         \begin{align}
%         e_t=I(\mathcal{|U}_E(t)|<K), \forall t.
%         \end{align}}
%     \end{itemize}
% \end{Policy}
% The corresponding value function of policy $\Pi$ is given by 
% \begin{align}
%     W_{\Pi}(\widetilde {\mathbf S})=W_{\Pi}^{(1)}(\widetilde {\mathbf S})+W_{\Pi}^{(2)}(\widetilde {\mathbf S})+W_{\Pi}^{(3)}(\widetilde {\mathbf S}),
%     \end{align}
% where $W_{\Pi}^{(1)}(\widetilde {\mathbf S})$, $W_{\Pi}^{(2)}(\widetilde {\mathbf S})$ and $W_{\Pi}^{(3)}(\widetilde {\mathbf S})$ are defined as \eqref{eqn:W1}, \eqref{eqn:W2} and \eqref{eqn:W3}, respectively.
% \begin{align}
% 	&W_{\Pi}^{(1)}(\widetilde{\mathbf{S}})\triangleq  \mathbb{E}_{\{\mathbf{S}_t|\forall t\},\{T_{k}|\forall k\}}^{\Pi} \bigg[ \sum_{k=1}^{|\mathcal{U}_E(1)|-K+1}
% 	\
% 	\sum_{t=\sum_{i=1}^{k-1}T_{i}+1}^{\sum_{i=1}^{k}T_{i}}\gamma^{t-1}{g}'(\mathbf{S}_t,\Pi(\mathbf{S}_t)) \bigg|\widetilde{\mathbf{S}}_1=\widetilde{\mathbf{S}}
% 	\bigg],\label{eqn:W1}\\
% 	&W_{\Pi}^{(2)}(\widetilde{\mathbf{S}})\triangleq  \mathbb{E}_{\{\mathbf{S}_t|\forall t\},\{T_{k}|\forall k\}}^{\Pi} \bigg[ \sum_{k=|\mathcal{U}_E(1)|-K+2}^{|\mathcal{U}_E(1)|}
% 	\ 
% 	\sum_{t=\sum_{i=1}^{k-1}T_{i}+1}^{\sum_{i=1}^{k}T_{i}}\gamma^{t-1}{g}'(\mathbf{S}_t,\Pi(\mathbf{S}_t)) \bigg|\widetilde{\mathbf{S}}_1=\widetilde{\mathbf{S}}
% 	\bigg],\label{eqn:W2}\\
% 	&W_{\Pi}^{(3)}(\widetilde{\mathbf{S}})\triangleq \lim\limits_{T\to +\infty} \mathbb{E}_{\{\mathbf{S}_t|\forall t\},\{T_{k}|\forall k\}}^{\Pi} \bigg[ 
% 	\sum_{t=\sum_{i=1}^{|\mathcal{U}_E(1)|}T_{i}+1}^{T}
% 	\gamma^{t-1}{g}'(\mathbf{S}_t,\Pi(\mathbf{S}_t)) \bigg|\widetilde{\mathbf{S}}_1=\widetilde{\mathbf{S}}
% 	\bigg].\label{eqn:W3}
% 	\end{align} 

% The expressions of $W_{\Pi}^{(1)}(\widetilde {\mathbf S})$, $W_{\Pi}^{(2)}(\widetilde {\mathbf S})$ and $W_{\Pi}^{(3)}(\widetilde {\mathbf S})$ are given by the following lemmas.
% \begin{Lemma}[Analytical Expression of $W_{\Pi}^{(1)}(\mathbf {\widetilde S})$]\label{lem:W1}
%     $W_{\Pi}^{(1)}(\widetilde{\mathbf{S}})$ can be written as 
%     \begin{align}\label{eq:W_Pi_1}
%         W_{\Pi}^{(1)}(\widetilde{\mathbf{S}}) = & \mathbb{E}_{\{T_{k}|\forall k\}}\left [ \sum_{ k =1}^{|\mathcal{U}_E(1)|-K+1}\gamma^{\sum\limits_{i=1}^{k-1}T_{i}} \left(  \frac{1-\gamma^{T_{k}}}{1-\gamma} \frac{p_r}{\rho_{m_{k}}} + w\Big(|\mathcal{U}_E(1)|-k+1\Big)\frac{1-\gamma^{\sum\limits_{i=1}^{k}T_{i}}}{1-\gamma}\right)\right ]\nonumber\\
%         &+P_N \mathbb{E}_{\substack{\{T_{k}|\forall k\}\\ \{C(n_t)|\forall t\leq \sum_{k=1}^{|\mathcal{U}_E(1)|-K+1} T_k \}}}\left[\sum_{t=1}^{\sum_{k=1}^{|\mathcal{U}_E(1)|-K+1}T_{k}} \gamma^{t-1}C(n_t) \right].
%         \end{align}	
% where 
% \begin{align}
% \mathbb{E} [C(n_t)]=\sum_{d_{min}}^{d_{max}}\frac{\int\int \pi_f(f)\pi_{\ell}(\ell)C(n_t)dfd\ell}{d_{max}-d_{min}+1}. 
% \end{align}
% Moreover, for sufficiently large input data size, we have
% \begin{align}\label{eq:Tk}
% T_{k} = \left\lceil\frac{ Q_{m_{k}} b_s }{\mathbb{E}_{
% 	h} W\log_2 \left( 1 + \frac{p_r |h|^2}{\sigma_z^2} \right)T_s} \right\rceil, \forall k,
% \end{align}
% where $ \mathbb{E}_{h}  $ is the expectation w.r.t. small-scale fading.
% \end{Lemma}

% \begin{Lemma}[Analytical Expression of $W_{\Pi}^{(2)}(\mathbf {\widetilde S})$]\label{lem:W2}
% 	Define the following notations:
% 	\begin{itemize}
% 		\item $\mathbf u=[ 0\ 0\ \dots 1\ 0]^{\mathbf T} \in \mathbb R^{(K+1)\times 1}$.
% 		\item $ \mathbf{g} = [g_1 \ g_2 \ ... \ g_{K+1}]^{\mathbf{T}} \in \mathbb R^{(K+1)\times 1}$, where $g_1=0$, and $g_i=w(i-1)+P_N\mathbb{E}[C(n_t)]$,  $\forall i=2,3,...,K+1$.
% 		\item $\mathbf{P}\in  \mathbb R^{(K+1)\times(K+1) } $, where $[\mathbf{P}]_{i,i-1}=1$, $\forall i=2,3,...,K+1$, $[\mathbf{P}]_{i,i}=1$ and
% 		other entries are all $0$.
% 		\item  $\mathbf{M}\in  \mathbb R^{(K+1)\times(K+1) } $, where $[\mathbf{M}]_{j,j+1}=P_N$, $[\mathbf{M}]_{j,j}=1-P_N$, $\forall j=1,2,...,K$, $[\mathbf{M}]_{K+1,K+1}=1$,  and
% 		other entries are all $0$.
% 	\end{itemize}
% 	Then, the analytical expression of 	$W_{\Pi}^{(2)}(\widetilde{\mathbf{S}})$ is given by
% 	\begin{align}\label{eqn:W_2}
% 	W_{\Pi}^{(2)}(\widetilde{\mathbf{S}}) =&  \mathbb{E}_{\{T_{k}|\forall k\}}\left [ 
% 	\sum_{k=|\mathcal{U}_E(1)|-K+2}^{|\mathcal{U}_E(1)|}
% 	\gamma^{\sum\limits_{i=1}^{k-1}T_{i}} \left(  \frac{1-\gamma^{T_{k}}}{1-\gamma} \frac{p_r}{\rho_{m_{k}}}\right)\right ]\nonumber\\
% 	&+	\sum_{k=|\mathcal{U}_E(1)|-K+2}^{|\mathcal{U}_E(1)|} \ \sum_{t=\sum_{i=1}^{k-1}T_{i}+1}^{\sum_{i=1}^{k}T_{i}} \gamma^{t-1}\mathbf{u}_{k}^{\mathbf{T}}(\mathbf{M})^{\beta_{k,t}}\mathbf{g},
% 	\end{align}
% 	where $\beta_{k,t} \triangleq t-\sum_{i=1}^{k-1}T_{i}-1$, and 
% 	\begin{align*}
% 	 &\mathbf{u}_{|\mathcal{U}_E(1)|-K+2}=\mathbf{u}, \\
% 	&\mathbf{u}_k\triangleq\left[ \mathbf{u}_{k-1}^{\mathbf{T}}(\mathbf{M})^{T_{k-1}}\mathbf{P}\right]^{\mathbf{T}}, \ k = |\mathcal{U}_E(1)|-K+3, \dots, |\mathcal{U}_E(1)|.
% 	\end{align*}	
% \end{Lemma}

% \begin{Lemma}[Analytical Expression of $W_{\Pi}^{(3)}(\mathbf {\widetilde S})$]\label{lem:W3}
% 	Define the following notations:
% 	\begin{itemize}
% 		\item $\zeta\in\{0,1,\dots,K\}$ denotes the number of edge computing devices.
% 		\item $\xi\in \{0,1,\dots,d_{\max}\}$ denotes the segment number of the first edge computing devices.
% 		\item $\epsilon_{\zeta,\xi}$ denotes the index of entry, and
% 		\begin{equation}
% 		\epsilon_{\zeta,\xi} \triangleq \left\{ \begin{array}{cc}
% 		1&  \zeta=0,\ \xi=0, \\
% 		(\zeta-1)d_{\max}+\xi+1	&  \mbox{otherwise.}
% 		\end{array}   \right.
% 		\end{equation}
% 		\item $\mathbf{v}\in \mathbb R^{(Kd_{\max}+1)\times 1}$, and the entries of $\mathbf{v}$ is given by 
% 		\begin{equation} \label{eqn:v}
% 			[\mathbf{v}]_{	\epsilon_{\zeta,\xi} }\triangleq \left\{ \begin{array}{cc}
% 			\left[\mathbf{u}_{|\mathcal{U}_E(1)|}^{\mathbf{T}}(\mathbf{M})^{T_{|\mathcal{U}_E(1)|}}\mathbf{P}\right]_{1}		&  \epsilon_{\zeta,\xi}=1, \\
% 			\frac{1}{d_{\max}-d_{\min}+1}\left[\mathbf{u}_{|\mathcal{U}_E(1)|}^{\mathbf{T}}(\mathbf{M})^{T_{|\mathcal{U}_E(1)|}}\mathbf{P}\right]_{i}		&  \zeta=i-1, \ d_{\min}\leq\xi\leq d_{\max
% 			},
% 			\\
% 			0	&  \mbox{otherwise.}
% 			\end{array}   \right.
% 			\end{equation}
	
% 		\item $\mathbf{c}\in \mathbb R^{(Kd_{\max}+1)\times 1}$, and 
% 		\begin{equation}
% 		[\mathbf{c}]_{	\epsilon_{\zeta,\xi} }\triangleq \left\{ \begin{array}{cc}
% 		0		&  \epsilon_{\zeta,\xi}=1, \\
% 		w\zeta+\mathbb{E}_{\rho_{n_t}}[\frac{p_r}{\rho_{n_t}}]	&  0<\zeta<k,
% 		\\
% 		w\zeta+\mathbb{E}_{\rho_{n_t}}[\frac{p_r}{\rho_{n_t}}]+P_N\mathbb{E}[C(n_t)]		&  \zeta=k.
% 		\end{array}   \right.
% 		\end{equation}.
% 	\end{itemize}
%     Then, the analytical expression of 	$W_{\Pi}^{(3)}(\widetilde{\mathbf{S}})$ is given by 
    
% 	\begin{align}\label{eq:W-Pi-3}
%         W_{\Pi}^{(3)}(\widetilde{\mathbf{S}})=\lim\limits_{T\to +\infty}\sum_{t=\sum_{i=1}^{|\mathcal{U}_E(1)|}T_{i}+1}^{T} \gamma^{t-1} \mathbf{v}^{\mathbf{T}}({\bf \Phi })^{t-\sum_{i=1}^{|\mathcal{U}_E(1)|}T_{i}-1}
%         \mathbf{c}=\gamma^{\sum_{i=1}^{|\mathcal{U}_E(1)|}T_{i}}\mathbf{v}^{\mathbf{T}}\left( \mathbf{I}-\gamma{\bf \Phi} \right)^{-1}\mathbf{c}
%         .
%     \end{align}        
%     where the entries of the transition probability matrix ${\bf \Phi}\in \mathbb{R}^{(Kd_{\max}+1)\times (Kd_{\max}+1)}$ are given in table \ref{table:Phi}, where $\alpha(x)=[2^{\frac{xb_s}{WT_s}}-1]\sigma_z^2$, other entries are all $0$.
% \end{Lemma}

% \begin{table*}
% 	\centering  % 显示位置为中间
% 	\caption{ENTRIES OF MATRIX ${\bf \Phi}$}  % 表格标题 
% 	\label{table:Phi}  % 用于索引表格的标签
% 	%字母的个数对应列数，|代表分割线
% 	% l代表左对齐，c代表居中，r代表右对齐
% 	\begin{tabular}{|c|c|c|c|c|}  
% 		\hline  % 表格的横线
% 		& & & & \\[-6pt]  %可以避免文字偏上来调整文字与上边界的距离
% 		$\zeta$&$\xi$&${\zeta}'$ & ${\xi}'$ & $[{\bf \Phi}]_{\epsilon_{\zeta,\xi},\ \epsilon_{{\zeta}',{\xi}'}}$ \\  % 表格中的内容，用&分开，\\表示下一行
% 		\hline
% 		0 & 0& 0 &0 & $1-P_N$
% 		\\
% 		\hline
% 		0 & 0 & 1& $d_{\min},\dots,d_{\max}$ & $\frac{P_N}{d_{\max}-d_{\min}+1}$ \\
% 		\hline
% 		$1,\dots,K-1$ & $1,...,d_{\max}$  & $\zeta$ & $1,...,\min(\xi,d_{\min}-1)$ & $(1-P_N)\left(\exp\{-\frac{\alpha(\xi-{\xi}')}{p_r}\}-\exp\{-\frac{\alpha(\xi-{\xi}'+1)}{p_r}\right)$
% 		\\
% 		\hline
% 		$1,\dots,K-1$ & $1,...,d_{\max}$  & $\zeta-1$ & $d_{\min}, \dots, d_{\max}$ & $\frac{1-P_N}{d_{\max}-d_{\min}+1}\left(\exp\{-\frac{\alpha(\xi)}{p_r}\}\right)$
% 		\\
% 		\hline
% 		$1,\dots,K-1$ & $1,...,d_{\max}$  & $\zeta+1$ & $1,...,\xi$ & $P_N\left(\exp\{-\frac{\alpha(\xi-{\xi}')}{p_r}\}-\exp\{-\frac{\alpha(\xi-{\xi}'+1)}{p_r}\right)$
% 		\\
% 		\hline
% % 		$1,\dots,K-1$ & $1,...,d_{\max}$  & $\zeta$ & $\xi+1,\dots,d_{\max}$ & $\frac{P_N}{d_{\max}-d_{\min}+1}\left(\exp\{-\frac{\alpha(\xi)}{p_r}\}\right)$
% 		\\
% 		\hline
% 		$1,\dots,K-1$ & $1,...,d_{\max}$  & $\zeta$ & $d_{\min},...,\xi
%         $ & $(1-P_N)\left(\exp\{-\frac{\alpha(\xi-{\xi}')}{p_r}\}-\exp\{-\frac{\alpha(\xi-{\xi}'+1)}{p_r}\right)
%         \atop +\frac{P_N}{d_{\max}-d_{\min}+1}\left(\exp\{-\frac{\alpha(\xi-1)}{p_r}\}\right)$
% 		\\
% 		\hline
% 		$K$ & $1,...,d_{\max}$  & $K$ & $1,...,\xi$ & $\left(\exp\{-\frac{\alpha(\xi-{\xi}')}{p_r}\}-\exp\{-\frac{\alpha(\xi-{\xi}'+1)}{p_r}\right)$
% 		\\
% 		\hline
% 		$K$ & $1,...,d_{\max}$  & $K-1$ & $d_{\min}, \dots d_{\max}$ & $\frac{1}{d_{\max}-d_{\min}+1}\exp\{-\frac{\alpha(\xi)}{p_r}\}$
% 		\\
% 		\hline
% 	\end{tabular}
% \end{table*}

% \begin{table*}
% 	\centering  % 显示位置为中间
% 	\caption{ENTRIES OF MATRIX $\frac{ \mathrm{d}{\bf \Phi} }{\mathrm{d}p_r}$}  % 表格标题 
% 	\label{table:derivative_Phi}  % 用于索引表格的标签
% 	%字母的个数对应列数，|代表分割线
% 	% l代表左对齐，c代表居中，r代表右对齐
	
% 		\begin{tabular}{|c|c|c|c|c|}  
% 			\hline  % 表格的横线
% 			& & & & \\[-6pt]  %可以避免文字偏上来调整文字与上边界的距离
% 			$\zeta$&$\xi$&${\zeta}'$ & ${\xi}'$ & $[
% 			\frac{ \mathrm{d}{\bf \Phi} }{\mathrm{d}p_r}]_{\epsilon_{\zeta,\xi},\ \epsilon_{{\zeta}',{\xi}'}}$ \\  % 表格中的内容，用&分开，\\表示下一行
% 			\hline
		
% 			$1,\dots,K-1$ & $1,...,d_{\max}$  & $\zeta$ & $1,...,\min(\xi,d_{\min}-1)$ & $(1-P_N)\Big(\frac{\alpha(\xi-{\xi}')}{p_r^2}\exp\{-\frac{\alpha(\xi-{\xi}')}{p_r}\}-\frac{\alpha(\xi-{\xi}'+1)}{p_r^2}\exp\{-\frac{\alpha(\xi-{\xi}'+1)}{p_r}\}\Big)$
% 			\\
% 			\hline
% 			$1,\dots,K-1$ & $1,...,d_{\max}$  & $\zeta-1$ & $d_{\min}, \dots, d_{\max}$ & $\frac{1-P_N}{d_{\max}-d_{\min}+1}\left(
% 			\frac{\alpha(\xi)}{p_r^2}\exp\{-\frac{\alpha(\xi)}{p_r}\}\right)$
% 			\\
% 			\hline
% 			$1,\dots,K-1$ & $1,...,d_{\max}$  & $\zeta+1$ & $1,...,\xi$ & $P_N\left(\frac{\alpha(\xi-{\xi}')}{p_r^2}\exp\{-\frac{\alpha(\xi-{\xi}')}{p_r}\}-\frac{\alpha(\xi-{\xi}'+1)}{p_r^2}\exp\{-\frac{\alpha(\xi-{\xi}'+1)}{p_r}\right)$
% 			\\
% 			\hline
% 			$1,\dots,K-1$ & $1,...,d_{\max}$  & $\zeta$ & $\xi+1,\dots,d_{\max}$ & $\frac{P_N}{d_{\max}-d_{\min}+1}\left(\frac{\alpha(\xi)}{p_r^2}\exp\{-\frac{\alpha(\xi)}{p_r}\}\right)$
% 			\\
% 			\hline
% 			$1,\dots,K-1$ & $1,...,d_{\max}$  & $\zeta$ & $d_{\min},...,\xi
% 			$ & $ (1-P_N)\left(\frac{\alpha(\xi-{\xi}')}{p_r^2}\exp\{-\frac{\alpha(\xi-{\xi}')}{p_r}\}-\frac{\alpha(\xi-{\xi}'+1)}{p_r^2}\exp\{-\frac{\alpha(\xi-{\xi}'+1)}{p_r}\right)
% 			\atop
% 			+\frac{P_N}{d_{\max}-d_{\min}+1}\left(
% 			\frac{\alpha(\xi)}{p_r^2}\exp\{-\frac{\alpha(\xi-1)}{p_r}\}\right)$
% 			\\
% 			\hline
% 			$K$ & $1,...,d_{\max}$  & $K$ & $1,...,\xi$ & $\left(\frac{\alpha(\xi-{\xi}')}{p_r^2}\exp\{-\frac{\alpha(\xi-{\xi}')}{p_r}\}-\frac{\alpha(\xi-{\xi}'+1)}{p_r^2}\exp\{-\frac{\alpha(\xi-{\xi}'+1)}{p_r}\right)$
% 			\\
% 			\hline
% 			$K$ & $1,...,d_{\max}$  & $K-1$ & $d_{\min}, \dots d_{\max}$ & $\frac{1}{d_{\max}-d_{\min}+1}\frac{\alpha(\xi)}{p_r^2}\exp\{-\frac{\alpha(\xi)}{p_r}\}$
% 			\\
% 			\hline
% 		\end{tabular}
	
% \end{table*}

% Then, the SGD algorithm to optimize $p_r$ is revised accordingly. The main difference of the SGD algorithm compared with previous version is the probability transition matrix $\mathbf \Phi$ and its derivative $\frac{d\mathbf\Phi}{dp_r}$ whose non-zero entries are given by table \ref{table:Phi} and table \ref{table:derivative_Phi}, respectively.

% After generalizing the baseline policy to the more general policy, the performance of the baseline policy and the corresponding proposed policy are both improved as shown by simulation. {\color{red} (add simulation results here...)}

% }


\spaceblank
\noindent{\em \textbf{Comment 4:} The system model is stuffed with known stuff, be it task offloading or local computing model. Hence, I do not see enough contributions to it.
}
\spaceblank
{\blue \textbf{Response R1-4:} Thank you for your comment. As explained in \textbf{Response R1-1}, we are considering a new uplink offloading scenario with random user arrivals, and hence the scheduling problem is totally different from the existing works. Consequently, the spatial and temporal distributions of active device arrivals modeled in Section II-A are not mentioned in the existing works of MEC system scheduling, e.g., \cite{You2015SingleUserWPT,you2016singleuser,huang2012dynamic,liu2016delayopt,you2016multiuser,chen2015decentalizedgame,chen2016gametheorymec,mao2016power-delay,Ko2018HetnetMEC}. On the other hand, the task offloading and local computing models, which are widely used and validated in existing literatures, are adopted in this work. These models are consistent with the existing works.
}


\spaceblank
\noindent{\em \textbf{Comment 5:} The same goes for low complexity scheduling. It also contains mostly known stuff. Please justify your contributions to it.
}
\spaceblank
{\blue \textbf{Response R1-5:} Thank you for your comment. There might be some misunderstanding on the solution framework proposed in this paper. As mentioned in \textbf{Response R1-1}, the mathematical problem in this work and our proposed solution framework are totally different from existing works. Typical low-complexity approaches to solve large-scale MDP problems, such as linear approximation \cite{YCui2010AMDP-OFDM-downlink,YCui2010AMDP-OFDM-uplink,RWang2011DistTowHopMIMO,RWang2011QueueAwareCoorp, RWang2013RelayApproxMDP,YSun2019PushingCaching,BLv2019Cache} and deep reinforcement learning \cite{XQiu2019DRL-MEC,LTan2018DRL-Cache-MEC,JWang2019ResorceAlloc-MEC,YLiu2019DRL-VehicleEdge,YHe2018cache-comp-DRL}, are not directly applicable to our problem. The comparison is made below.
\begin{itemize}
\item \textbf{Different approximation methods:} In the conventional low-complexity approximate MDP solution \cite{YCui2010AMDP-OFDM-downlink,YCui2010AMDP-OFDM-uplink,RWang2011DistTowHopMIMO,RWang2011QueueAwareCoorp, RWang2013RelayApproxMDP,YSun2019PushingCaching,BLv2019Cache}, the global system state (represents the status of the overall system) is decoupled into the aggregation of a number of local system states (each represents partial status of the system). Then the value function with optimal policy (which is a function of global system state) is approximately decomposed into a linear combination (summation) of local value functions (each is a function of local system state). Each local value function is usually evaluated numerically via iterative algorithm (namely value iteration). However, the decoupling of value function (of the optimal policy) to the linear combination of local value functions may lose its original structure, which is usually not linear. In other words, the linear combination of local value functions does not represent any physical meaning of the system. Based on this observation, we try to improve the value function approximation in this work. In our proposed approximate MDP method, we can derive the expression of value function with a baseline policy, and use it as the approximation of value function with the optimal policy (namely approximate value function). The advantages of this new method are: (1) The approximate value function has clear physical meaning. It represents an achievable performance (with the baseline policy); (2) We can derive the analytical expression of the approximate value function and  do not rely on numerical iterations, which reduces the complexity of value function calculation. 

\item \textbf{Analytical performance bound:} We can obtain an analytical bound of the proposed algorithm with our new method. For conventional approximate MDP methods \cite{YCui2010AMDP-OFDM-downlink,YCui2010AMDP-OFDM-uplink,RWang2011DistTowHopMIMO,RWang2011QueueAwareCoorp, RWang2013RelayApproxMDP,YSun2019PushingCaching,BLv2019Cache}, it is hard to investigate the performance analytically due to the use of approximate value function. In our proposed method, however, it is proved that the derived value function of the baseline policy (approximate value function) is a cost upper bound of the proposed policy. Thus, the worst-case performance of the proposed policy can be obtained analytically. 
\end{itemize}
}


\spaceblank
\noindent{\em \textbf{Comment 6:} How you guarantee the low complexity?
}
\spaceblank
{\blue \textbf{Response R1-6:} Thank you for your comment. 
Compared with optimal MDP solution and existing approximate MDP solution, our proposed method can significantly reduce the complexity in the phase of value iteration. 

Take the uplink transmission with $Q$ fixed users as an example. Suppose each queue can buffer at most $P$ packets. For the value iteration in optimal solution, there are $P^Q$ states and the complexity is $\mathcal O(P^QN_G)$, where $N_G$ is the number of global iterations until convergence. For the value iteration in approximate MDP, if the global system state is decoupled to $Q$ local system states, the computation complexity is $\mathcal O(PQN_L)$, where $N_L$ is the number of local iterations until convergence. For our proposed scheduling algorithm, the complexity of value function calculation for an arbitrary system state is $\mathcal O(1)$ since we can obtain the analytical expression of the approximate value function.

Moreover, given the derived approximate value function, the complexity of the one-step policy iteration in our work is $\mathcal O(N_p|\mathcal U_E(t)|)$, where $N_p$ is the number of quantization levels of transmit power and $|\mathcal U_E(t)|$ is the number of active edge computing devices. Also, we have modified the manuscript and added the discussion on the complexity of our proposal.

}


\spaceblank
\noindent{\em \textbf{Comment 7:} Simulations results are also very limited. I would like to see more results with important inferences.
}
\spaceblank
{\blue \textbf{Response R1-7:} Thank you for your comment. Since we have proposed a new baseline policy, all the simulation results have been re-generated. Moreover, we have also added new figures to demonstrate more performance insights as shown in Fig. \ref{fig:cost_size} and Fig. \ref{fig:mismatch} (Fig. 6 and Fig. 12 in the manuscript). Fig. \ref{fig:cost_size} shows the performance gain of the new baseline policy and the corresponding sub-optimal scheduling policy compared with the counterparts in the conference version. Fig. \ref{fig:mismatch} demonstrates the necessity of the reinforcement learning algorithm by showing the performance gain benefiting from the accurate learning of the arrival rate of active devices.
}


\spaceblank
\noindent{\em \textbf{Comment 8:} There are some grammatical mistakes and typos. Please read the paper carefully.
}
\spaceblank
{\blue \textbf{Response R1-8:} Thank you for your comment. We have carefully checked the grammatical mistakes and typos.
}


\section{Response to Reviewer 2}
\spaceblank
\noindent{\em \textbf{Comment 1:} The authors have considered a wireless channel model where the small-scale fading coefficient $h_k(t)$ is complex Gaussian distributed with zero mean and variance 1. But to the best of my knowledge, a typical mobile edge computing scenario involves relative motion in between the Tx and Rx, which induces the concept of Doppler frequency. But this work does not deal with Doppler, as the channel is arbitrarily taken as i.i.d. Hence, either the notion of Doppler should be introduced or its omission needs to be properly justified with appropriate references.
}
\spaceblank
{\blue \textbf{Response R2-1:} Thank you for your comment. We would like to clarify the channel model from the following aspects. 
\begin{itemize}
	\item The i.i.d. block fading channel model is widely adopted in existing works in mobile edge computing. For example, all the works in \cite{huang2012dynamic,mao2016dynamicmec,mao2016power-delay,liu2016delayopt, Ko2018HetnetMEC} assume the channel fading is i.i.d. in each frame and the locations of mobile devices are quasi-static. Our channel model simply follows these works.
	\item In this paper, we consider the scenario that the mobile devices' locations are quasi-static during the uplink transmission, or their mobility is sufficiently small such that their large-scale fading to the BS is quasi-static. In this scenario, the small-scale (multipath) fading coefficients will still vary from frame to frame due to the motion of scatterers (e.g. cars and human bodies) or the small-scale motion of the mobile devices. Thus, there is Doppler effect. It is elaborated in Section 3.3.3 of \cite{goldsmith_2005} that 
	
	\emph{``In particular $A_C(\Delta t = T ) = 0$ indicates that observations of the channel impulse response at times separated by $T$ are uncorrelated and therefore independent, since the channel is a Gaussian random process. We define the channel coherence time $T_c$ to be the range of values over which $A_C(\Delta t)$ is approximately nonzero. Thus, the time-varying channel decorrelates after approximately $T_c$ seconds. The function $S_C(\rho)$ is called the Doppler power spectrum of the channel: as the Fourier transform of an autocorrelation it gives the PSD of the received signal as a function of Doppler $\rho$. The maximum $\rho$ value for which $|S_C(\rho)|$ is greater than zero is called the Doppler spread of the channel, and is denoted by $B_D$. By the Fourier transform relationship between $A_C(\Delta t)$ and $S_C(\rho)$, $B_D \approx 1/T_c$."} 

	\noindent Thus, given the Doppler spread $B_d$ (which depends on the motion of channel scatters and mobile devices), the small-scale fading coefficients become independent every $1/B_d$ seconds, where $1/B_d$ is usually referred to as channel coherent time. In our channel model, the frame duration is approximately $1/B_d$.
\end{itemize}
}


\spaceblank
\noindent{\em \textbf{Comment 2:} In the scheduling policy three action items namely $a_t$, $p(t)$ and $e_t$ have been used. The authors may put some light to the size of the action space considered, whether continuous space is considered for first two action items or not. Also how does the size of the action space affect the complexity of the system or not.
}
\spaceblank
{\blue \textbf{Response R2-2:} Thank you for your comment. The cardinalities for $a_t$ and $e_t$ are $|\mathcal U_E(t)|$ and 2, respectively, where $a_t$ represents the selected transmission device and $e_t$ represents the offloading decision. Moreover, although the action space of transmission power $p(t)$ is continuous, it can be quantized such that each transmission device can choose from a number of discrete power levels in each frame. As mentioned in \textbf{Response R1-6}, the complexity of our proposed one-step policy iteration algorithm is $\mathcal O(N_p|\mathcal U_E(t)|)$, where $N_p$ is the number of quantization levels of transmit power. We have modified the manuscript and added the discussion on the complexity of our proposal.
}


\spaceblank
\noindent{\em \textbf{Comment 3:} Minor correction: In page 3, line 18, “is quasi-statistic in cell” may be corrected to “is quasi-static in cell”.
}
\spaceblank
{\blue \textbf{Response R2-3:} Thank you for your comment. We have corrected this typo.
}

\section{Response to Reviewer 3}
\spaceblank
\noindent{\em \textbf{Comment 1:} This paper studies the task scheduling problem in mobile edge computing system. The authors propose a reinforcement learning based approach to jointly optimize the task offloading, uplink transmission device selection, and power allocation. The paper is well written. However, the novelty of this paper is not enough since this paper is too similar to their published conference paper. The new contribution mainly comes from some practical assumption, which is not enough for this high-quality journal.
}
\spaceblank
{\blue \textbf{Response R3-1:} Thank you for your comment. Firstly, we would like to clarify that the practical consideration of unknown user distribution is not our contribution, but an issue which should be addressed. One of our new contributions in this journal is to propose a novel technique to address the above issue under the proposed approximate MDP solution framework. Moreover, to further improve the performance, we have revised the manuscript essentially by proposing a better baseline policy compared with the original manuscript. The power adaptation algorithm and reinforcement learning algorithm are also revised accordingly. Specifically, compared with the conference paper, the main contributions of this manuscript are summarized below. 

% In the revised manuscript, we have developed a new baseline policy and derive its value function. This new baseline policy is more general and has better performance than the baseline policy in our conference version. In the policy of conference version, the BS can only allow one offloading device at any time; and the number of offloading devices is generalized to arbitrary number in the revised manuscript. Significant efforts are spent to derive the value function of the new baseline policy, as it is different from the conference version. It is shown in simulation section that the new baseline policy leads to better performance after one-step policy iteration (i.e., the proposed policy).

\begin{itemize}
	\item \textbf{A new baseline policy with better performance.} In the revised journal version, we extend the baseline policy of the conference version to a more general policy. In the baseline policy of the conference version, at most one active edge computing device is allowed in the system. Thus, a new active device is scheduled for edge computing if and only if there is currently no edge computing device in the uplink queue. While in the revised journal version, we generalize the baseline policy by allowing at most $K$ active edge computing devices in the uplink queue. This generalization is non-trivial as we should consider the transition probabilities between different numbers of edge computing devices (there is no such issue in the conference version as only one edge computing device is allowed in the baseline policy of the conference version). Significant efforts are spent to derive the value function of the new baseline policy. Simulation result in Fig. \ref{fig:cost_size} (Fig. 6 of the revised manuscript) demonstrates that the revised baseline policy with $K=4$ has better performance than the original baseline policy ($K=1$). Better baseline policy also leads to better proposed policy after one-step policy iteration. For example in Fig. \ref{fig:cost_size}, the proposed policy with $K=4$ (obtained via one-step policy iteration from the new baseline policy with $K=4$) is better than the proposed policy with $K=1$ (obtained via one-step policy iteration from the original baseline policy). Moreover, the new baseline policy can also provide a tighter analytical cost upper bound on the proposed policy.
	
	
	
	\item \textbf{An efficient algorithm to optimize the transmission power of the baseline policy without knowledge of user distribution statistics.} The performance of the baseline policy,  as well as the proposed policy evolved from the baseline policy,  depends heavily on the uplink transmission power. We develop a stochastic gradient descent (SGD) algorithm to optimize the transmission power without system statistics in an online manner, such that the performance of both baseline and proposed policies can be improved. In the conference version, we fixed the uplink transmission power. The performance gain of this optimization is illustrated in Fig. \ref{fig:cost_pr} (Fig. 11 of the revised manuscript), where the optimized $p_r^*$ lead to better performance in both baseline policy and the proposed policy.
	
	\item \textbf{An efficient reinforcement learning algorithm without knowledge of user distribution statistics.} In this work, the approximate value function depends on the statistics of new active devices which may not be known to the BS in practice. In the conference version, we do not consider this issue, and simply make assumptions on the distribution of new active devices. This will cause the distribution mismatch if the assumption is not accurate. In order to address this issue, we design a novel and efficient reinforcement learning method for evaluating the value function in the journal version. The conventional reinforcement learning method, i.e. Q-learning, needs to learn the Q-function value for all state-action pairs, which is infeasible in our problem due to the tremendous state and action spaces. In the proposed reinforcement learning method, we only need to track the statistical parameters in the expression of approximate value function via online observations. The learning efficiency is significantly improved. Moreover, the learning of statistical parameters can greatly improve the performance of the proposed policy. For example, the performance gain from tracking the user arrival rate is illustrated in Fig. \ref{fig:mismatch} (Fig. 12 of the revised manuscript), where the curve with $P_N=0.2$ refers to the proposed policy with initial inaccurate value of $P_N$, and the curve with $P_N=0.1$ refers to the proposed policy with learned accurate value of $P_N$.
	\end{itemize}
% In addition to the above revision, the new contribution compared with the conference version also include:
% \begin{itemize}
% \item We propose an efficient reinforcement learning algorithm to evaluate the value function without statistical knowledge of user distribution. We would like to clarify that our contribution is not on introducing this practical assumption, but on proposing novel technique to address this practical assumption. In conventional reinforcement learning algorithms, the value functions need to be evaluated for all state-action pairs (e.g., Q-learning method) or at least a large subset of state-action pairs (i.e., approximate MDP method). Thus, the scheduler needs to traverse a sufficiently large number of states for many times, which results in large computation complexity and slow convergence rate. In our proposed reinforcement learning algorithm, however, we only need to learn some unknown parameters of the value function which are common for all system states. This is because we have the analytical expression of the approximate value function. It is easy to see that the convergence time will be significantly shortened.
% \item We propose a stochastic optimization algorithm to adjust the transmission power of the baseline policy without statistical knowledge of user distribution. The performance of the baseline policy,  as well as the proposed policy evolved from the baseline policy,  depends heavily on the transmission parameter. We design a stochastic gradient descent (SGD) algorithm to optimize the transmission parameter with unknown system statistics in an online manner, such that the performance of both baseline and proposed policies can be improved.
% \end{itemize}
}


\spaceblank
\noindent{\em \textbf{Comment 2:} The authors mention that the studied problem is modeled as an infinite-horizon MDP. The authors proposed an approximate MDP and utilize the reinforcement learning to solve it. Then, what is the size of the action space for the reinforcement learning approach? The authors are also suggested to make some analysis about the complexity of the proposal.
}
\spaceblank
{\blue \textbf{Response R3-2:} Thank you for your comment. In this paper, the structure of the proposed solution is as follows. 
\begin{itemize}
\item \textbf{Step 1:} We first introduce a baseline policy and derive its value function;
\item \textbf{Step 2:} Then, we adopt one-step policy iteration to obtain the proposed scheduling algorithm;
\item \textbf{Step 3:} When the arrival statistics of active devices are unknown, an efficient reinforcement learning algorithm is proposed to track the unknown parameters in the value function expression derived in the first step. Moreover, we also track the unknown parameters in the gradient of value function expression with respect to transmission power, such that the transmission power level can also be optimized.
\end{itemize}

In Step 1, since the value function of the baseline policy is derived, the computation complexity for calculating the value function for an arbitrary system state is O(1). In Step 2, the scheduling policy consists of three actions, namely $a_t$, $p(t)$ and $e_t$. The space cardinalities for $a_t$ and $e_t$ are $|\mathcal U_E(t)|$ and 2, respectively, where $a_t$ represents the selected transmission device and $e_t$ represents the offloading decision. Moreover, although the action space of transmission power $p(t)$ is continuous, it can be quantized such that each transmission device can choose from a number of discrete power levels in each frame. As mentioned in \textbf{Response R1-6}, the complexity of our proposed one-step policy iteration algorithm is $\mathcal O(N_p|\mathcal U_E(t)|)$, where $N_p$ is the number of quantization levels of transmit power. In the Step 3, since we have the analytical expression of the value function, we do not need to resort to conventional Q-learning algorithms where tremendous number of iterations are required to obtained the state-action value function (Q-function). Instead, we only need to learn the expectation of some random variables from their unbiased observations. The MSE of such observation is $\mathcal O(1/N_o)$, where $N_o$ is the number of online observations. We have modified the manuscript and added the discussion on the complexity of our proposed algorithm.
}


\spaceblank
\noindent{\em \textbf{Comment 3:} Some important research on edge computing should be surveyed and discussed to improved the quality of this paper. I just list some works as below:\\
"Online Deep Reinforcement Learning for Computation Offloading in Blockchain-Empowered Mobile Edge Computing," in IEEE Transactions on Vehicular Technology, vol. 68, no. 8, pp. 8050-8062, Aug. 2019.\\
"Mobility-Aware Edge Caching and Computing in Vehicle Networks: A Deep Reinforcement Learning," in IEEE Transactions on Vehicular Technology, vol. 67, no. 11, pp. 10190-10203, Nov. 2018.\\
"Smart Resource Allocation for Mobile Edge Computing: A Deep Reinforcement Learning Approach," in Transactions on Emerging Topics in Computing.\\
"Deep Reinforcement Learning for Offloading and Resource Allocation in Vehicle Edge Computing and Networks," in IEEE Transactions on Vehicular Technology.\\
"Integrated Networking, Caching, and Computing for Connected Vehicles: A Deep Reinforcement Learning Approach," in IEEE Transactions on Vehicular Technology, vol. 67, no. 1, pp. 44-55, Jan. 2018.
}
\spaceblank
{\blue \textbf{Response R3-3:} Thank you for your comment. We have added the references above and discussed them in the Introduction section.
}



\bibliographystyle{IEEEtran}
\bibliography{reference_shortened.bib}		
\end{document}